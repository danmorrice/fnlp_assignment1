lm_stats=[5579336, 0.3116466822608252, 0.9853501906482038, 1.4536908944718608e-06, 0.005158178014182952, 0.348077988641086]
dev_tweets_preds=[False, False, False, False, False, True, True, False, False, False, False, False, True, False, False, False, True, False, False, True, False, True, True, True, True, False]
answer_short_1_3='"\nLeft and right padding adds tokens to signify the start and end of words which provides more context for the model, \nallowing it to better understand the structure and placements of letters in a word. A string ending in “oq” will have a\nhigher entropy with padding than without. Although the bigram is not unlikely, any word ending in “oq” will have a large entropy as \n‘q’ will rarely end a word.'
answer_short_1_4='"\np(b|(\'<s>\',)) = [2-gram] 0.046511   Probability of letter ‘b’ coming at start of a word  \np(b|(\'b\',)) = [2-gram] 0.007750     Probability of letter ‘b’ coming after letter ‘b’\nbacking off for (\'b\', \'q\')          ‘q’ coming after letter ‘b’ is never seen in bigram so back off to unigram\np(q|()) = [1-gram] 0.000892         Probability of seeing letter ‘q’ in unigram\np(q|(\'b\',)) = [2-gram] 0.000092     Adjusted probability of ‘q’ coming after letter ‘b’ using back-off weight*unigram\np(</s>|(\'q\',)) = [2-gram] 0.010636  Probability of the letter \'q\' coming at the end of a word\n7.85102054894183                    Cross entropy'
answer_short_1_5='There are two main collections in the plots: \n    - centred around the mean\n    - high entropy skewed to the right\n\nLower entropy tweets are likely in English since the model was trained on an English corpus and so it has more \nconfidence in its predictions. Tweets with high entropy are likely non-English. We can classify English and \nnon-English tweets using some entropy threshold which divides these classes.'
top10_ents=[(2.4921691054394848, ['and', 'here', 'is', 'proof', 'the']), (2.5390025889056127, ['and', 'bailed', 'he', 'here', 'is', 'man', 'on', 'that', 'the']), (2.5584079236733106, ['is', 'the', 'this', 'weather', 'worst']), (2.568653427817313, ['s', 's', 's', 's', 's', 's', 's', 's', 's', 's']), (2.569853705187651, ['be', 'bus', 'here', 'the', 'to', 'want']), (2.576919752608039, ['hell', 'that', 'the', 'was', 'wat']), (2.587767243678531, ['creation', 'is', 'of', 'on', 'story', 'the', 'the']), (2.5885860368906832, ['fro', 'one', 'the', 'the', 'with']), (2.595298329492654, ['is', 'money', 'motive', 'the', 'the']), (2.617870705175611, ['at', 'bucks', 'end', 'lead', 'of', 'the', 'the', 'the'])]
bottom10_ents=[(17.523736748003564, ['作品によっては怪人でありながらヒーロー', 'あるいはその逆', 'というシチュエーションも多々ありますが', 'そうした事がやれるのもやはり怪人とヒーローと言うカテゴリが完成しているからだと思うんですよね', 'あれだけのバリエーションがありながららしさを失わないデザインにはまさに感服です']), (17.524868750262904, ['ロンブーの淳さんはスピリチュアルスポット', 'セドナーで瞑想を実践してた', 'これらは偶然ではなく必然的に起こっている', '自然は全て絶好のタイミングで教えてくれている', 'そして今が今年最大の大改革時期だ']), (17.5264931699585, ['実物経済と金融との乖離を際限なく広げる', 'レバレッジが金融で儲けるコツだと', 'まるで正義のように叫ぶ連中が多いけど', 'これほど不健全な金融常識はないと思う', '連中は不健全と知りながら', '他の奴がやるから出し抜かれる前に出し抜くのが道理と言わんばかりに群がる']), (17.527615646393077, ['一応ワンセット揃えてみたんだけど', 'イマイチ効果を感じないのよね', 'それよりはオーラソーマとか', '肉体に直接働きかけるタイプのアプローチの方が効き目を感じ取りやすい', '波動系ならバッチよりはホメオパシーの方がわかりやすい']), (17.53293217459052, ['慶喜ほどの人でさえこうなんだから', '並の人間だったらなおさら参謀無しじゃ何も出来ない', '一般に吹聴されてる慶喜のネガティブ論は', 'こうした敵対勢力による相次ぐテロに対して終始無関心で', '慶喜個人だけに批判を向けがち']), (17.541019489814225, ['昨日のセミナーではお目にかかれて光栄でした', '楽しく充実した時間をありがとうございました', '親しみのもてる分かりやすい講演に勇気を頂きました', '素晴らしいお仕事とともに益々のご活躍願っております', '今後ともよろしくお願いします']), (17.541411086467402, ['自民党が小沢やめろというなら', '当然町村やめろというブーメランがかえってくるわけです', 'おふたりとも選挙で選ばれた正当な国民の代表ですから', 'できればどちらにもやめてほしくありません', 'そろそろこんな不毛なことはやめにしてほしい']), (17.5427257173663, ['知識欲というのは不随意筋でできている', 'どうせ人間には永久に解明できないんだから', '宇宙はある時点で生まれたのか', 'それとも永遠の過去から存在しているのかなんてことを追究するなと言ってもムダだ', '心臓に止まれと命令しても止まらないのと同じことだ']), (17.547644050965395, ['と言いつつもやっぱり笑えない時はあるよなあ', '笑っても自分の笑顔が汚らわしく思えてすぐ止めちゃうの', '自分が息してるだけで悲しくてぼろぼろ泣いてる時期もあった', '今の自分に必要な経験だったとは思うけど', '出来ればあんな感情は二度とごめんだ']), (17.55280652132174, ['中身の羽毛は精製過程で殺菌処理しているから', '羽毛布団からダニが湧くことはない', 'あと羽毛布団の生地は糸の打ち込み本数が多く', '羽毛の吹き出しを防ぐ目つぶし加工をしているからダニは羽毛ふとんの生地を通過できない', 'ただダニが布団に付着することはあるから手入れは必要'])]
answer_essay_question='Three problems that the question glosses over:\n\nWhat kind of language model will be used? To make entropy calculations, we need a language model that can predict the next word. \nLarger N-gram models provide more context than smaller N-grams, making more accurate predictions of the entropy values. \nThis comes with a higher computational cost, however.\n\nWhat corpus is used to train the model, and what is the size of this corpus? Depending on the source of this data (is it from news articles,\nor social media), the context will be very different and so word predictions and entropies will vary. For example, if news articles are used \nthen the language model will be formal, contrasting a more informal language model if social media was used. A larger corpus will also give a \nmore reliable estimate as it has more to learn from.\n\nIs punctuation included or not? Punctuation being included or not can massively influence a model’s prediction for the next word. For example, \na period can signify the end of a sentence and thus impact the next word for the start of the sentence. Along these same lines, is left and right \npadding included, to signify the start and end of sentences as well?\n\n\n\nI would begin my experiment by collecting a large, formal corpus of English. I would opt for a formal corpus over \nsomething such as social media, as it is well-formed and devoid of slang, thus representing the English language better. \nI would use several sources from different areas to account for language variations.\n\n    I would preprocess this corpus by removing special characters and punctuation (as it does not contribute much to word-level entropies) \nand converting text to lowercase. This makes it easier for the model to process. Then, I would tokenise this corpus, using the words as tokens, \nand add left and right padding to the sentences. Then I would split the corpus so that 20% of it was saved for testing purposes.\n    \nI would train a tri-gram language model on this tokenised corpus. This provides more accuracy in entropy calculations than a bigram or \nunigram (due to increased context), but it is more computationally expensive. \nThe tri-gram equation used is as follows:       \n$P_{MLE}(w_i|w_{i-2},w_{i-1})=\x0crac{C(w_{i-2},w_{i-1},w_i)}{C(w_{i-2},w_{i-1})}$\n\n    Finally, I would calculate the entropy for each word in the corpus using the following entropy formula:\n        $H(X)=\\Sigma_{x}-P(x)log_{2}P(x)$                   '
naive_bayes_vocab_size=13521
naive_bayes_prior={'V': 0.47766934282005674, 'N': 0.5223306571799433}
naive_bayes_likelihood=[0.006913064743371673, 0.0012190937826220374, 0.12333945519182665, 2.2315401420605136e-06, 2.6766530157370086e-05, 0.004917741586185903, 0.004933935254095795]
naive_bayes_posterior=[{'V': 0.5886037204341036, 'N': 0.4113962795658964}, {'V': 0.15633267093793746, 'N': 0.8436673290620625}, {'V': 0.8124219037837196, 'N': 0.18757809621628035}, {'V': 0.8124219037837196, 'N': 0.18757809621628035}, {'V': 0.9961437486715607, 'N': 0.003856251328439274}]
naive_bayes_classify=['V', 'N']
naive_bayes_acc=0.7949987620698192
answer_open_question_2_2='1. \nUsing individual features provides a low accuracy. P (preposition) gives the highest accuracy on its own which suggest it is the most i\nnformative for the task.\n\nBy combining all features there is a substantial increase in accuracy. This suggests that the features capture some different aspects \nof the data which are relevant to classifying whether the PP attaches to NP or VP.\n\n\n2. \nAccuracy from 2.1 = 79.50%.\nNaive Bayes assumes the inputs are conditionally independent given the class, whilst the logistic regression \ndoes not make the same assumptions, allowing it to capture more complex relations.\n\n\n3. \nI would advocate against this feature – it is extremely specific, capturing only a tiny subset of the data and would not generalise \nfor data which does not contain these words, leading to extreme overfitting. It would only work if a sentence contains these words in this order.'
lr_predictions='VVVVVVVVVVVVVVVVVVVVVVVVVVVVVNVVVVVVVVNVNVVVNVNNNNVVNVVNNNVNNNVNVVVVNNVVVNVNVVNVNVNVNNNVVVNVNNNVNVNVNNVNVVVNNVNNVNNNNVVVNVVNVNVVNNVNNVNVNNNVVNVNNVNNNNVNNNNNVNNNNVNNNNVNNNNNNNNNVVVVVVVNVNVNNNNVVVVNVVNVVVNVVNNVVNVNVNNVNVNVVVNVNVNNNNVNVNVNNVNNVNNVNNVNVVNNNNVNVNVNNVNVVNNNNVVNVNVNVVNVVVNVVNNNNVNNVVNNNVNNVVNNNNNVNVNNVNNNNNVVVVNVVVVVVVNNNNVVVNNNNNVNVNVNNVVNVNNNNNVNVNVVNVVVNNNVVNNVVVVVVNNNNNNVVNVVNNNNNNVNNVVVNNVVVVVVVVNNNVNVNVVVNNVVVVVVVVVNVVVVNVNVNNVVNVVVNVNNVNNVVNVNNNVNVNNVVVNVNNVNNNVNNNVNVNVVVVVVNVVNVNNVNNVNNVVVNVNNVVVVVVVVVVVVVVVVVNVVVNVNNVVNVVNNNNVNVNNNVNVNVNVVNNNNNNVVNNNNVVVVVNNVNVNNNNVNVVNVNVVNVNVVVVVVVNVNNNVVNNVVVNNNNVNVVVVVNVNVVVNVVVVNVVVNVNVVVVNNVNVVNVVNVVNNNNVVNNVNVNVVNNVVVVVVNVVVVVVVVVVVVVVVVNVNVVNNVVVVVVVVNNNNNVVNNVNVNNNNVVVVNVVVNNVNVVNVVVNVNNNNNNNVNNVNVNNVVVVNNNNNNVVVNVVVVVVVVNVNVNVNVNVNVVNVNNNVVNVNNNVNVNVNVVNVNNVVNNNNVNNVVNVVVNVVVVNVNVVNVNNNNNNNNVNVNVNNNNVVNNVNNVVVVVNVNNNNVNNNNVNVVVNNNNNVVNVVNNNVNNVNNNNNNVNNNVNVVVNVVVNVVNNVNNNNVNVVVVNVNVNNNVVVNNNNVNVNNNNNNNVVVVVVVNVVVVVNVNVVVVNNVVVNNNVVVNVVNNVNVNNNNNVVVNNNNVVVVVNVNVNVVVNNVNNVNVVNNNVNVNVNNVNVNVVVVVVVNNVNVNNVVVNNNNNVNNNNVNVNNVNNNNNNVVNNVVNNNNNNVVVVVNVNNNVNVNNNVVVVVVVVVNVVVNVNNVNVVNNNNVVNVVNVVVNVVVVVNNNNNNNNNNNNNNVNNVNNNVNNVNNVVVVNVVNVNNNNVVNVVVNNVNNNVVVNNNNVVNNNVNNNVNNNNVVVNVVNVNNNVVVNVNNNVNVVVVVNVNNNNVVNNNNNVVNNVNNNNNNVVVVNNVVVNVNVNNVVVNVNNVVVNVVVVVNVNNVVVNVNVNVNVVVNVVVVVVVVNNVNNNNVNVVNNNVVNNVNNVNVNNVVVVVNNNVVVNNVVVVNNNNVVVNNVVVVVNNNNNNVVVVNVNVVVVVNVNNNNVVNVVVNNVNVVNNVNNNNNNNNVNNNVNNNNVNNNVNNVNNVVNVVNNVNNVVNNNNNNVNNVVNNNVNVNNVNNNVVVNNNNVNNVNVNNNVNNVNNVNNNNNVNNVNNVNNNNVVNVNVVNNVNVNVNVNNVVVVVVNNVNNVNNVVVVNNNVVVVVVNVNVNVNNVNNNNNNVNVNVNNVVNNVNNNVNVVVNNVNNNVNVNVNNVVNVVNVVVNVNVVVVNNNNVVNNVVNNNVNVNNNVVNVNNVNVNVVNNVNVVVVVNVVVNNNVNNNVNNNNNNNNNNNNNVVVNVVVNVVVNNVNVVNVNVVVNNVVNNNNVVNVNVNVNVNVNVVVNVNNNNVVNVNNVNNVNNNNNNNVNVNVNNVNVVVVVNVVVNNNNNVNNNVNNNVNNVVNNVVNVNNNNNVNNVNVNNVNVVNNNVVVNVNVVVVNNVNVNVVVVVNNNVNNNNNVNVNNNNVNVVNNNNNNVNVNVVVVNVVNNNNVNNVNVVNNNNNVVNNVNVNNNNVNNVVNNVNNVNVNNVNVNVVVVNVNVNVNVVVVNVVVVVNVNVNNNVVNNVVNVNVNVNNVVNNNVNNNNNNVNVNNVVNNVNVNNNNNNVNVVVNNNVVVNNNVVVVVNVVVNNVNVNNVVNNNVVNVVNNVNVVVNVVVNVVVVNNVVVNNVVNNVVVVNNNNNVNVVNNNNNNVNVVVNVVNVNVVNNNVNVVNVNVVNNVVVVNNNVVVNNVNVVNNVNNVVVNVNVNVNVNNVVVVVVVVNNNVNVNNNVNVNNNNNNVVVNNNNVNNVVNNVVNVNNVNNVVNVNNNNVVVNNNVNVVVNVNNVVVVNNNNVVVVVNVVNVVVNNNVVVNVVNNVVVNNNVNNNNVNVNNVVNVNNVVNNNNNVNVVNVVNVNVNNVVVVNNVNVNVVVVVNVNNVVNVNNNNNVVNNVNNNVNNNNNVNVVVNVVVNNVVNNNNNVVVNNVVVVNNVVNVVNNVNVNNNNNVVVNNNNNVNNVNNVVVVVNVNVVVVVVVNNNVNNNVNNVNVVNVNVNVNNVVVNNNVNNNNNNNVVNNVNVNNNNVVVVVVNVNVNNVVNVNVVNVNVVVVNNVNNVVNNVVVVNNNVVVVNNVVVNNNVVVNNVVVNNNNNVVVNVNVVNNNVNNVVNVVVNNNNVNVVVVVVVVNNNVNVVNNNNVNVVNNVVVNNNNVVVNVNNVNNNNNVVVNVVNNVNVNNVNVVVVNVNVVVVVVVVNVNNNNVVVNVNVNVVNNVVNVVNVNVNVNNNVVVNNNVVVVVNNNNNVVVNNVNVVVVVVNNVNNVVNNVNVVNNNNVNNNVNNVVVVVNVVVVNVVVVVNNNVNNNVVVNVVVVVNVVNNVNNNVVNNVNVVVVVVNNVVVVVVVNVVVVNVVVNNVNNNVVVVNVVVVNNNVNVNNVVNNNVVNNVNNVNVNVNNVNVNNNNNVNNNVVVVNNNNNNNNNVNVNNVNNNNNNNVNVVNNNNVVVVVNVNNNVVVNVNVVNVVNNNNNNNVVVVVVNVVVVVNVVVVVNNVVNNVVNVVVVNNNNNNVVNVVNVVVNNNNVVNNVNNVVVVVVVNVVVNVNVNVNVVVVNVNVNVNVNVNNNNNNVVVNVVVNNVNNVNNNVNVNNNNNNNVVVVNVNVNVVVVVVVVVVVNVVVVVNNNVNNVVNVVVVNNVNVNNVVVVNNNNVVVVVVNNNNNVNVNNNVNNNVVNVNVVVVVNNVNVNNVNNVVNNNNVVVVVVVVVNVVVVNVVNNVNVNNNNNVNVVVVNVNNVVVVNNVNNNNVNNVVVNVNVVNNNVVNNNVVVVNVVNNVNNNVVNVVNVNVNNVVNNNVNNVNNVVVVNNVNVNNNVNVNVNVVVVVNVNVNNVNNVNNVVVNNNVNNVVVVVVNVNVVVVVVNVNVVVVNNVVVNNVVNVNNVVNNNVNVVNNNVVVNVNNNNVNNNVVVVVVVVVVNVVVNNNNNVVVNVVNNNVNVNNNNNNNVVNVVNNNVVVVVVNNNNVVVNVVNVNVNNVVVVNNVNNVNVVNNVNNNNVNVVNNNNVVVNNNVVNVVNNNNVVVVVVVNVVNNVVNNNNVVVVVVNNVVVVNNVNNVNVNNNVNVVNVVNNNNNVNVVNNNVVVNVNNNVVVVVVNVVVNNVNVNNNNNNVNVVNVVNVVVVVVNNNNNVVNNNNVVVVVVVNNVNVVNNNVVVNNNVVNNVVNVVNVVVNNVNNVVVVVNVVNNVNNVVVVVVVNNVVVNNNNNNVVNNVNVVVNNNNVVNNVNNVNVNVVVVNNNNVNVVNNNVNVNNVVNVVVNNNVVNNVNVVNNNVNVNVNVNVNVVVVVNNNNVVNVNNNVNNNVNNNNNNNVNVVVVNNNVNNNNNNNNVVNVNVVNVVNVNNNVNNNNNNNNNNVNNNVNVNVNVNVNVNNVNVNVNVNVNVVVNNNVNVNNNNNVNNNNNNVNVVVVNNVNVNNNNNNVVNVNNVVNNVNVVVVVNVVNNNVVNVVVNNVVNNNVNVNVVNVVNNVNVVVVVVNNNNNNNNNVNVNNNVNNNNNNNNVNVNNNVVVVNVNVNVNVNVNVVVNNNNNNVVVVNNNNNNNNVNNVVVNVVVNVVVNVVNNVVVVNNVNVNVNNVNVVNVNNVVVVVVNVVNVVNVNNVN'
best10_features=
answer_open_question_2_3="Started by adding all of the pairs of features to test the performance of this, and it improved the accuracy. My intuition here was \nthat the lone features might be indicative of a certain class, but by combining features into pairs, I could allow the model to capture the more complex \nrelationships between classes and features that would otherwise be missed.\n\nBecause of the success with pairwise combinations, I tried implementing the function with triplets and quadruplets of the features. However, by adding \nmany of these, I decreased the accuracy of the model – I believe the increased number of features caused the model to overfit the training data. \n\nI tweaked my feature templates, removing some of the pairs and adding a triplet of features instead. My intuition here was that, similarly to the \npairs, adding a triplet may allow the model to capture even more complex relationships between features and classes. By removing some pairs, I ensured \nit wasn’t too complex and so didn’t overfit. After some trial and error, I managed to get the highest accuracy using this at 84.67%.\n\n1:\n“-5.680 ('p', 'of')==1 and label is 'V'”\nThis feature is informative as it has the strongest absolute weight of all the features. The presence of this feature suggests a strong negative correlation \nwith the label “V”, meaning it is often found with the label “N”.\n\n2:\n“3.539 ('vp', ('assume', 'of'))==1 and label is 'V'”\nThis feature is informative as it has the strongest correlation with the label “V”. The presence of the verb “assume” now makes it indicative of a verb phrase.\n\n3:\n“2.625 ('n1n2', ('3', 'point'))==1 and label is 'N'”\nStrong positive correlation with a noun phrase. This makes sense as a quantity (such as 3) can only be attributed to a noun, not a verb."